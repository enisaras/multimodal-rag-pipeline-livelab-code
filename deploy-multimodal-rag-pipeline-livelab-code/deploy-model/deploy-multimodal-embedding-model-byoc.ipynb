{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ead758",
   "metadata": {},
   "source": [
    "# Deploy Multimodal Embedding Model with OCI Data Science BYOC\n",
    "This notebook is supplemental to [Deploy a Multimodal RAG Pipeline On OCI Data Science and Generative AI]() Livelab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b93c6-2171-48c8-85a8-7e8ef9c80205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages and setup auth\n",
    "import ads\n",
    "import os\n",
    "ads.set_auth(\"resource_principal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6c1bb",
   "metadata": {},
   "source": [
    "### Setting up infrastructure variables needed for deployment\n",
    "\n",
    "Before you can run this notebook, you need to set the following variables:\n",
    "\n",
    "**region**: Region to deploy model infrastructure and deployment in. Set by ads package.\n",
    "\n",
    "**container_image**: The path to your container image that was pushed to OCIR in Lab 3.\n",
    "\n",
    "**compartment_id**: Compartment where the project was deployed in. One of the environment variables set automatically in OCI Data Science Notebooks.\n",
    "\n",
    "**project_id**: OCID for the project. Set automatically by an environment variable in OCI Data Science Notebooks.\n",
    "\n",
    "**log_group_id**: Optional, Log group OCID that was obtained at the end of lab 4, task 1.\n",
    "\n",
    "**log_id**: Optional, log OCID that was obtained at the end of lab 4, task 2.\n",
    "\n",
    "**instance_shape**: The instance that the multimodal embedding model will be deployed on. GPU shapes are recommend for larger models or highly concurrent requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608aa9f-b21f-4185-bbae-af6fd8b71aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract region information from the Notebook environment variables and signer\n",
    "region = ads.common.utils.extract_region()\n",
    "# Replace container image to your container image path\n",
    "container_image = \"<your-container-image-path>\"\n",
    "\n",
    "# Set environment variables\n",
    "compartment_id = os.environ[\"PROJECT_COMPARTMENT_OCID\"]\n",
    "project_id = os.environ[\"PROJECT_OCID\"]\n",
    "\n",
    "# Optional logging resources\n",
    "log_group_id = \"<your-log-group-ocid>\"\n",
    "log_id = \"<your-log-ocid>\"\n",
    "\n",
    "# Specify instance shape\n",
    "instance_shape = \"VM.GPU.A10.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d58a5-d158-4cf7-8dfe-a4e27add4baf",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Multimodal embedding models can process text and images the same. There are many open source examples on HuggingFace that we can use.\n",
    "\n",
    "The following code cells download the embedding model to local storage, and uploads it to an object storage bucket. Second cell creates an OCI Data Science Model from reference to the specified object storage bucket.\n",
    "\n",
    "You will need to set the following variables:\n",
    "\n",
    "**bucket**: Bucket name that we created earlier this lab.\n",
    "\n",
    "**namespace**: Tenant namespace OCID that we obtained in Lab 3, Task 3.\n",
    "\n",
    "**model_prefix**: A prefix for your model from HuggingFace.\n",
    "\n",
    "**Note**: You may need to authenticate with HuggingFace for some models that are gated by repository owners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bucket= \"<your-bucket-name>\" # this should be a versioned bucket\n",
    "namespace = \"<your-tenant-namespace-id>\"\n",
    "model_name = \"<your-model-name>\" # HuggingFace model name, usually: <model-provider>/<model-name>\n",
    "model_prefix = \"<model-prefix>\" # e.g VLM2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download $model_name --local-dir $model_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oci os object bulk-upload --src-dir $model_name --prefix $model_prefix -bn $bucket -ns $namespace --auth \"resource_principal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02928c16-7dd1-4993-800e-5f92053044f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ads.model.datascience_model import DataScienceModel\n",
    "bucket = \"<your-bucket-name>\"\n",
    "namespace = \"<your-tenant-namespace>\"\n",
    "model_prefix = \"<model-prefix>\"\n",
    "artifact_path = f\"oci://{bucket}@{namespace}/{model_prefix}\"\n",
    "\n",
    "model = (DataScienceModel()\n",
    "  .with_compartment_id(compartment_id)\n",
    "  .with_project_id(project_id)\n",
    "  .with_display_name(f\"{model_prefix}\")\n",
    "  .with_artifact(artifact_path)\n",
    ")\n",
    "\n",
    "model.create(model_by_reference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0becf6",
   "metadata": {},
   "source": [
    "### Configure infrastructure details\n",
    "The cell below configures the infrastructure details for the Data Science BYOC deployment. This deployment will be placed behind a load balancer. Some definitions:\n",
    "\n",
    "**with_bandwith_mbps**: By default, this is set to 10. If you higher bandwidth requirements, you can scale this up as needed.\n",
    "\n",
    "**with_replica**: Amount of instances to create and load the model. If you have high concurrency requirements, you can scale this up accordingly.\n",
    "\n",
    "**with_access_log** and **with_predict_log**: The access logs control who accesses your model, and predict logs are logs emitted from the container image deployed. This is optional but highly recommended. You may delete these variables if you are not using logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4a82cba-4045-43f7-9912-fdd5aa2e1224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ads.model.deployment import (\n",
    "    ModelDeployment,\n",
    "    ModelDeploymentContainerRuntime,\n",
    "    ModelDeploymentInfrastructure,\n",
    "    ModelDeploymentMode,\n",
    ")\n",
    "infrastructure = (\n",
    "    ModelDeploymentInfrastructure()\n",
    "    .with_project_id(project_id)\n",
    "    .with_compartment_id(compartment_id)\n",
    "    .with_shape_name(instance_shape)\n",
    "    .with_bandwidth_mbps(10)\n",
    "    .with_replica(1)\n",
    "    .with_web_concurrency(1)\n",
    "    .with_access_log(\n",
    "        log_group_id=log_group_id,\n",
    "        log_id=log_id,\n",
    "    )\n",
    "    .with_predict_log(\n",
    "        log_group_id=log_group_id,\n",
    "        log_id=log_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a755f",
   "metadata": {},
   "source": [
    "### Configure container runtime details\n",
    "\n",
    "The code cell below configures runtime details for our container. Some definitions:\n",
    "\n",
    "**with_env**: Environment variables to set on the container. Since we are deploying an embedding model, we will use the /v1/embeddings endpoint and this will to the predict endpoint of the model deployment.\n",
    "\n",
    "**with_cmd**: Container startup command, these are arguments we are adding to vLLM.\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48203d6e-9c1e-4a11-810f-349687fdd717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "env_var = {\n",
    "    'MODEL_DEPLOY_PREDICT_ENDPOINT': '/v1/embeddings',\n",
    "}\n",
    "\n",
    "cmd_var = [\"--model\", f\"/opt/ds/model/deployed_model/{model_prefix}\", \"--tensor-parallel-size\", \"2\", \"--port\", \"8080\", \"--served-model-name\", \"odsc-llm\", \"--host\", \"0.0.0.0\", \"--trust-remote-code\"]\n",
    "\n",
    "container_runtime = (\n",
    "    ModelDeploymentContainerRuntime()\n",
    "    .with_image(container_image)\n",
    "    .with_server_port(8080)\n",
    "    .with_health_check_port(8080)\n",
    "    .with_env(env_var)\n",
    "    .with_cmd(cmd_var)\n",
    "    .with_deployment_mode(ModelDeploymentMode.HTTPS)\n",
    "    .with_model_uri(model.id)\n",
    "    .with_region(region)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89135f3",
   "metadata": {},
   "source": [
    "### Deploy Model\n",
    "Code cell below deploys the model with infrastructure and container runtime details we previously set. This will create the deployment and we can watch the deployment state by running the code cell after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3abad8-2577-4e36-9113-dd14a6a635bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment = (\n",
    "    ModelDeployment()\n",
    "    .with_display_name(f\"{model_prefix} MD with BYOC\")\n",
    "    .with_description(f\"Deployment of {model_prefix} MD with vLLM BYOC container\")\n",
    "    .with_infrastructure(infrastructure)\n",
    "    .with_runtime(container_runtime)\n",
    ").deploy(wait_for_completion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd319b76-572c-48af-837c-e450302994fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment.watch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6b777-52c7-47fc-a1df-0f6ab37f194f",
   "metadata": {},
   "source": [
    "If everything worked correctly, your model should now be deployed. Move to inference-with-byoc-model to test and experiment with multimodal embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db55600",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
